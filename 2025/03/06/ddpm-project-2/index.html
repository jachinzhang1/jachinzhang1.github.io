<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    
    <meta name="author" content="Jachin Zhang">
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

    
    <!--- Seo Part-->
    
    <link rel="canonical" href="https://jachinzhang1.github.io/2025/03/06/ddpm-project-2/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
    
    
        
        <meta name="description" content="Hexo Theme Redefine, Redefine Your Hexo Journey.">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习基础：基于DDPM模型的生成模型demo笔记（二）">
<meta property="og:url" content="https://jachinzhang1.github.io/2025/03/06/ddpm-project-2/index.html">
<meta property="og:site_name" content="Jachin&#39;s Blog">
<meta property="og:description" content="Hexo Theme Redefine, Redefine Your Hexo Journey.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://jachinzhang1.github.io/images/redefine-og.webp">
<meta property="article:published_time" content="2025-03-06T14:26:07.000Z">
<meta property="article:modified_time" content="2025-03-07T14:07:17.900Z">
<meta property="article:author" content="Jachin Zhang">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="图像生成">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jachinzhang1.github.io/images/redefine-og.webp">
    
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/my-favicon-universe.svg" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/my-favicon-universe.svg">
    <meta name="theme-color" content="#0098d7">
    <link rel="shortcut icon" href="/images/my-favicon-universe.svg">
    <!--- Page Info-->
    
    <title>
        
            深度学习基础：基于DDPM模型的生成模型demo笔记（二） | Jachin&#39;s Blog
        
    </title>

    
<link rel="stylesheet" href="/fonts/Chillax/chillax.css">


    <!--- Inject Part-->
    

    
<link rel="stylesheet" href="/css/style.css">


    
        
<link rel="stylesheet" href="/css/build/tailwind.css">

    

    
<link rel="stylesheet" href="/fonts/GeistMono/geist-mono.css">

    
<link rel="stylesheet" href="/fonts/Geist/geist.css">

    <!--- Font Part-->
    
    
    
    
    
    
        
<script src="/js/build/libs/anime.min.js"></script>

    

    <script id="hexo-configurations">
    window.config = {"hostname":"jachinzhang1.github.io","root":"/","language":"en","path":"search.xml"};
    window.theme = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center","image_caption":true,"link_icon":true,"delete_mask":true,"title_alignment":"left","headings_top_spacing":{"h1":"2.5rem","h2":"2.0rem","h3":"1.7rem","h4":"1.3rem","h5":"1.1rem","h6":"1.0rem"}},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":false,"auto":false,"list":[]},"code_block":{"copy":true,"style":"simple","highlight_theme":{"light":"github","dark":"vs2015"},"font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":3,"number":true,"expand":true,"init_open":true},"copyright":{"enable":true,"default":"cc_by_nc_sa"},"lazyload":true,"pangu_js":false,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"mobile_limit":2,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#0098d7","secondary":null,"default_mode":"light"},"global":{"fonts":{"chinese":{"enable":false,"family":null,"url":null},"english":{"enable":false,"family":null,"url":null},"title":{"enable":false,"family":null,"url":null}},"content_max_width":"1000px","sidebar_width":"210px","hover":{"shadow":true,"scale":false},"scroll_progress":{"bar":true,"percentage":true},"website_counter":{"url":"https://cn.vercount.one/js","enable":true,"site_pv":true,"site_uv":true,"post_pv":true},"single_page":true,"preloader":{"enable":true,"custom_message":null},"open_graph":{"enable":true,"image":"/images/redefine-og.webp","description":"Hexo Theme Redefine, Redefine Your Hexo Journey."},"google_analytics":{"enable":false,"id":null}},"home_banner":{"enable":true,"style":"fixed","image":{"light":"/images/day.png","dark":"/images/night.png"},"title":"Welcome to my sekai.","subtitle":{"text":["Stay hungry, stay foolish.","(๑＞ڡ＜)☆"],"hitokoto":{"enable":false,"show_author":false,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#000","dark":"#fff"},"text_style":{"title_size":"2.8rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":false,"family":null,"url":null},"social_links":{"enable":true,"style":"default","links":{"github":"https://github.com/jachinzhang1","instagram":null,"zhihu":null,"twitter":null,"email":"3023518860@qq.com","bilibili":"https://space.bilibili.com/518197475?spm_id_from=333.1007.0.0"},"qrs":{"weixin":"/images/qrs/weixin.png","qq":"/images/qrs/qq.png"}}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":false,"type":"fixed","audios":[{"name":null,"artist":null,"url":null,"cover":null,"lrc":null}]},"mermaid":{"enable":true,"version":"11.4.1"},"markdown-it-emoji":{"enable":true}},"version":"2.8.2","navbar":{"auto_hide":false,"color":{"left":"#f78736","right":"#367df7","transparency":35},"width":{"home":"1200px","pages":"1000px"},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"},"文章":{"path":"/archives","icon":"fa-regular fa-archive"},"分类":{"path":"/categories","icon":"fa-regular fa-list"},"标签":{"path":"/tags","icon":"fa-regular fa-tags"},"Links":{"icon":"fa-regular fa-link","submenus":{"GITHUB":"https://github.com/jachinzhang1","BiliBili":"https://space.bilibili.com/518197475?spm_id_from=333.1007.0.0","资源分享":"/links/","相册":"/masonry/"}}},"search":{"enable":true,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"blur"},"home":{"sidebar":{"enable":true,"position":"left","first_item":"menu","announcement":null,"show_on_mobile":true,"links":null},"article_date_format":"auto","excerpt_length":200,"categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":3}},"footerStart":"2025/2/2 00:00:00"};
    window.lang_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
    window.data = {"masonry":true};
  </script>
    
    <!--- Fontawesome Part-->
    
<link rel="stylesheet" href="/fontawesome/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/brands.min.css">

    
<link rel="stylesheet" href="/fontawesome/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/regular.min.css">

    
    
    
    
<meta name="generator" content="Hexo 7.3.0"></head>



<body>
	<div class="progress-bar-container">
	
	<span class="scroll-progress-bar"></span>
	

	
	<span class="pjax-progress-bar"></span>
	<!--        <span class="swup-progress-icon">-->
	<!--            <i class="fa-solid fa-circle-notch fa-spin"></i>-->
	<!--        </span>-->
	
</div>

<style>
    :root {
        --preloader-background-color: #fff;
        --preloader-text-color: #000;
    }

    @media (prefers-color-scheme: dark) {
        :root {
            --preloader-background-color: #202124;
            --preloader-text-color: #fff;
        }
    }

    @media (prefers-color-scheme: light) {
        :root {
            --preloader-background-color: #fff;
            --preloader-text-color: #000;
        }
    }

    @media (max-width: 600px) {
        .ml13 {
            font-size: 2.6rem !important; /* Adjust this value as needed */
        }
    }

    .preloader {
        display: flex;
        flex-direction: column;
        gap: 1rem; /* Tailwind 'gap-4' is 1rem */
        align-items: center;
        justify-content: center;
        position: fixed;
        padding: 12px;
        top: 0;
        right: 0;
        bottom: 0;
        left: 0;
        width: 100vw;
        height: 100vh; /* 'h-screen' is 100% of the viewport height */
        background-color: var(--preloader-background-color);
        z-index: 1100; /* 'z-[1100]' sets the z-index */
        transition: opacity 0.2s ease-in-out;
    }

    .ml13 {
        font-size: 3.2rem;
        /* text-transform: uppercase; */
        color: var(--preloader-text-color);
        letter-spacing: -1px;
        font-weight: 500;
        font-family: 'Chillax-Variable', sans-serif;
        text-align: center;
    }

    .ml13 .word {
        display: inline-flex;
        flex-wrap: wrap;
        white-space: nowrap;
    }

    .ml13 .letter {
        display: inline-block;
        line-height: 1em;
    }
</style>

<div class="preloader">
    <h2 class="ml13">
        Jachin&#39;s Blog
    </h2>
    <script>
        var textWrapper = document.querySelector('.ml13');
        // Split text into words
        var words = textWrapper.textContent.trim().split(' ');

        // Clear the existing content
        textWrapper.innerHTML = '';

        // Wrap each word and its letters in spans
        words.forEach(function(word) {
            var wordSpan = document.createElement('span');
            wordSpan.classList.add('word');
            wordSpan.innerHTML = word.replace(/\S/g, "<span class='letter'>$&</span>");
            textWrapper.appendChild(wordSpan);
            textWrapper.appendChild(document.createTextNode(' ')); // Add space between words
        });

        var animation = anime.timeline({ loop: true })
            .add({
                targets: '.ml13 .letter',
                translateY: [20, 0],
                translateZ: 0,
                opacity: [0, 1],
                filter: ['blur(5px)', 'blur(0px)'],
                easing: "easeOutExpo",
                duration: 1200,
                delay: (el, i) => 300 + 20 * i,
            })
            .add({
                targets: '.ml13 .letter',
                translateY: [0, -20],
                opacity: [1, 0],
                filter: ['blur(0px)', 'blur(5px)'],
                easing: "easeInExpo",
                duration: 1000,
                delay: (el, i) => 15 * i,
                complete: function() {
                    hidePreloader();
                }
            }, '-=700');


        let themeStatus = JSON.parse(localStorage.getItem('REDEFINE-THEME-STATUS'))?.isDark;

        // If the theme status is not found in local storage, check the preferred color scheme
        if (themeStatus === undefined || themeStatus === null) {
            if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
                themeStatus = 'dark';
            } else {
                themeStatus = 'light';
            }
        }

        // Now you can use the themeStatus variable in your code
        if (themeStatus) {
            document.documentElement.style.setProperty('--preloader-background-color', '#202124');
            document.documentElement.style.setProperty('--preloader-text-color', '#fff');
        } else {
            document.documentElement.style.setProperty('--preloader-background-color', '#fff');
            document.documentElement.style.setProperty('--preloader-text-color', '#000');
        }

        window.addEventListener('load', function () {
            setTimeout(hidePreloader, 5000); // Call hidePreloader after 5000 milliseconds if not already called by animation
        });

        function hidePreloader() {
            var preloader = document.querySelector('.preloader');
            preloader.style.opacity = '0';
            setTimeout(function () {
                preloader.style.display = 'none';
            }, 200);
        }
    </script>
</div>

<main class="page-container" id="swup">

	

	<div class="main-content-container flex flex-col justify-between min-h-dvh">
		<div class="main-content-header">
			<header class="navbar-container px-6 md:px-12">
    <div class="navbar-content transition-navbar ">
        <div class="left">
            
            <a class="logo-title" href="/">
                
                Jachin&#39;s Blog
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/"
                                        >
                                    <i class="fa-regular fa-house fa-fw"></i>
                                    HOME
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/archives"
                                        >
                                    <i class="fa-regular fa-archive fa-fw"></i>
                                    文章
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/categories"
                                        >
                                    <i class="fa-regular fa-list fa-fw"></i>
                                    分类
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/tags"
                                        >
                                    <i class="fa-regular fa-tags fa-fw"></i>
                                    标签
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="has-dropdown"
                                   href="#"
                                        onClick=&#34;return false;&#34;>
                                    <i class="fa-regular fa-link fa-fw"></i>
                                    LINKS
                                    <i class="fa-solid fa-chevron-down fa-fw"></i>
                                </a>

                                <!-- Submenu -->
                                
                                    <ul class="sub-menu">
                                        
                                            <li>
                                                <a target="_blank" rel="noopener" href="https://github.com/jachinzhang1">
                                                    GITHUB
                                                </a>
                                            </li>
                                        
                                            <li>
                                                <a target="_blank" rel="noopener" href="https://space.bilibili.com/518197475?spm_id_from=333.1007.0.0">
                                                    BILIBILI
                                                </a>
                                            </li>
                                        
                                            <li>
                                                <a href="/links/">
                                                    资源分享
                                                </a>
                                            </li>
                                        
                                            <li>
                                                <a href="/masonry/">
                                                    相册
                                                </a>
                                            </li>
                                        
                                    </ul>
                                
                            </li>
                    
                    
                        <li class="navbar-item search search-popup-trigger">
                            <i class="fa-solid fa-magnifying-glass"></i>
                        </li>
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fa-solid fa-magnifying-glass"></i>
                    </div>
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile sheet -->
    <div class="navbar-drawer h-dvh w-full absolute top-0 left-0 bg-background-color flex flex-col justify-between">
        <ul class="drawer-navbar-list flex flex-col px-4 justify-center items-start">
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/"
                        >
                            <span>
                                HOME
                            </span>
                            
                                <i class="fa-regular fa-house fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/archives"
                        >
                            <span>
                                文章
                            </span>
                            
                                <i class="fa-regular fa-archive fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/categories"
                        >
                            <span>
                                分类
                            </span>
                            
                                <i class="fa-regular fa-list fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/tags"
                        >
                            <span>
                                标签
                            </span>
                            
                                <i class="fa-regular fa-tags fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item-sub text-base my-1.5 flex flex-col w-full">
                        
                        <div class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary cursor-pointer text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                             navbar-data-toggle="submenu-Links"
                        >
                            <span>
                                LINKS
                            </span>
                            
                                <i class="fa-solid fa-chevron-right fa-sm fa-fw transition-all"></i>
                            
                        </div>
                        

                        
                            <div class="flex-col items-start px-2 py-2 hidden" data-target="submenu-Links">
                                
                                    <div class="drawer-navbar-item text-base flex flex-col justify-center items-start hover:underline active:underline hover:underline-offset-1 rounded-3xl">
                                        <a class=" text-third-text-color text-xl"
                                           target="_blank" rel="noopener" href="https://github.com/jachinzhang1">GITHUB</a>
                                    </div>
                                
                                    <div class="drawer-navbar-item text-base flex flex-col justify-center items-start hover:underline active:underline hover:underline-offset-1 rounded-3xl">
                                        <a class=" text-third-text-color text-xl"
                                           target="_blank" rel="noopener" href="https://space.bilibili.com/518197475?spm_id_from=333.1007.0.0">BILIBILI</a>
                                    </div>
                                
                                    <div class="drawer-navbar-item text-base flex flex-col justify-center items-start hover:underline active:underline hover:underline-offset-1 rounded-3xl">
                                        <a class=" text-third-text-color text-xl"
                                           href="/links/">资源分享</a>
                                    </div>
                                
                                    <div class="drawer-navbar-item text-base flex flex-col justify-center items-start hover:underline active:underline hover:underline-offset-1 rounded-3xl">
                                        <a class=" text-third-text-color text-xl"
                                           href="/masonry/">相册</a>
                                    </div>
                                
                            </div>
                        
                    </li>
            

            
            
        </ul>

        <div class="statistics flex justify-around my-2.5">
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/tags">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">8</div>
        <div class="label text-third-text-color text-sm">Tags</div>
    </a>
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/categories">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">4</div>
        <div class="label text-third-text-color text-sm">Categories</div>
    </a>
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/archives">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">15</div>
        <div class="label text-third-text-color text-sm">Posts</div>
    </a>
</div>
    </div>

    <div class="window-mask"></div>

</header>


		</div>

		<div class="main-content-body transition-fade-up">
			

			<div class="main-content">
				<div class="post-page-container flex relative justify-between box-border w-full h-full">
	<div class="article-content-container">

		<div class="article-title relative w-full">
			
			<div class="w-full flex items-center pt-6 justify-start">
				<h1 class="article-title-regular text-second-text-color tracking-tight text-4xl md:text-6xl font-semibold px-2 sm:px-6 md:px-8 py-3">深度学习基础：基于DDPM模型的生成模型demo笔记（二）</h1>
			</div>
			
		</div>

		
		<div class="article-header flex flex-row gap-2 items-center px-2 sm:px-6 md:px-8">
			<div class="avatar w-[46px] h-[46px] flex-shrink-0 rounded-medium border border-border-color p-[1px]">
				<img src="/images/luna_jp.png">
			</div>
			<div class="info flex flex-col justify-between">
				<div class="author flex items-center">
					<span class="name text-default-text-color text-lg font-semibold">Jachin Zhang</span>
					
				</div>
				<div class="meta-info">
					<div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2025-03-06 22:26:07</span>
        <span class="mobile">2025-03-06 22:26:07</span>
        <span class="hover-info">Created</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2025-03-07 22:07:17</span>
            <span class="mobile">2025-03-07 22:07:17</span>
            <span class="hover-info">Updated</span>
        </span>
    

    
        <span class="article-categories article-meta-item">
            <i class="fa-regular fa-folders"></i>&nbsp;
            <ul>
                
                
                    
                        
                        <li>
                            <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a>&nbsp;
                        </li>
                    
                    
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fa-regular fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
        <span class="article-wordcount article-meta-item">
            <i class="fa-regular fa-typewriter"></i>&nbsp;<span>4.6k Words</span>
        </span>
    
    
        <span class="article-min2read article-meta-item">
            <i class="fa-regular fa-clock"></i>&nbsp;<span>23 Mins</span>
        </span>
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

				</div>
			</div>
		</div>
		

		


		<div class="article-content markdown-body px-2 sm:px-6 md:px-8 pb-8">
			<h2 id="前情提要">前情提要</h2>
<p>本系列上篇博客中我们已经实现了UNet神经网络用于DDPM模型反向过程中的噪声预测，并成功在MNIST和CIFAR-10数据集上训练和测试。其中训练于MNIST数据集的模型生成情况良好，但训练于CIFAR-10数据集上的模型无法生成有意义的图片。</p>
<p>推测原因在于网络结构还不足以拟合多通道的复杂图像情形下的任务。因此我们打算加入自注意力机制增强模型对图像特征的提取，并试图将类别标签的特征融入模型使其可以实现类别条件的控制生成。此外，我们可以在训练过程中使用一些策略调整学习率，优化模型的训练过程。</p>
<h2 id="引入类别标签特征">引入类别标签特征</h2>
<p>将类别特征融入模型训练的方法有很多，不过受到最近读的某篇文章的启发，我打算将该特征融入时间步编码中，这样操作简单，无需改动整体的网络结构。</p>
<p>对<code>networks.py</code>的<code>Positiona0lEmbedding</code>类，展示改动后的结果：
<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                 max_seq_len: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 d_model: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 n_classes: <span class="built_in">int</span>=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Assume d_model is an even number for convenience</span></span><br><span class="line">        <span class="keyword">assert</span> d_model % <span class="number">2</span> == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># time step encoding</span></span><br><span class="line">        <span class="comment"># ...existing code...</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.t_embedding = nn.Embedding(max_seq_len, d_model)</span><br><span class="line">        <span class="variable language_">self</span>.t_embedding.weight.data = pe</span><br><span class="line">        <span class="variable language_">self</span>.t_embedding.requires_grad_(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># label encoding</span></span><br><span class="line">        <span class="variable language_">self</span>.use_condition = n_classes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_condition:</span><br><span class="line">            <span class="variable language_">self</span>.label_embedding = nn.Embedding(n_classes, d_model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, t, label=<span class="literal">None</span></span>):</span><br><span class="line">        t_emb = <span class="variable language_">self</span>.t_embedding(t)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_condition <span class="keyword">and</span> label <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            label_emb = <span class="variable language_">self</span>.label_embedding(label)</span><br><span class="line">            <span class="keyword">return</span> t_emb + label_emb</span><br><span class="line">        <span class="keyword">return</span> t_emb</span><br></pre></td></tr></table></figure></div>
为了区分时间步编码和类别标签编码，我们将时间步编码器更名为<code>t_embeddding</code>，类别编码器则是<code>label_embedding</code>，这两个编码器的输出分别是<code>t_emb</code>和<code>label_emb</code>。将所得的时间步编码和类别编码相加作为最终输出即可，该输出同时包含了两部分的特征（原理和残差网络有些类似）。</p>
<p>为此，神经网络要作一些微小的改动，即将标签输入包括进去。</p>
<p>首先是UNet初始化函数的输入： <div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                 n_steps,</span></span><br><span class="line"><span class="params">                 channels=[<span class="number">10</span>, <span class="number">20</span>, <span class="number">40</span>, <span class="number">80</span>],</span></span><br><span class="line"><span class="params">                 pe_dim=<span class="number">10</span>,</span></span><br><span class="line"><span class="params">                 residual=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 n_classes: <span class="built_in">int</span>=<span class="number">10</span></span>):</span><br></pre></td></tr></table></figure></div>
也就是加上一个类别的数量。找到该类中的<code>self.pe</code>的定义，改为：
<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable language_">self</span>.pe = PositionalEncoding(n_steps, pe_dim, n_classes)</span><br></pre></td></tr></table></figure></div> 然后是该类的<code>forward</code>方法： <div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, t, label=<span class="literal">None</span></span>):</span><br><span class="line">    n = t.shape[<span class="number">0</span>]</span><br><span class="line">    t = <span class="variable language_">self</span>.pe(t, label)</span><br><span class="line">    encoder_outs = []</span><br><span class="line">    <span class="comment"># ...existing code...</span></span><br></pre></td></tr></table></figure></div></p>
<p>最后在该模块的<code>build_network</code>函数中做如下更改：
<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">build_network</span>(<span class="params">n_steps, </span></span><br><span class="line"><span class="params">                  channels=<span class="literal">None</span>, </span></span><br><span class="line"><span class="params">                  pe_dim=<span class="literal">None</span>, </span></span><br><span class="line"><span class="params">                  residual=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                  n_classes=<span class="number">10</span></span>):</span><br><span class="line">    <span class="keyword">return</span> UNet(n_steps, channels, pe_dim, residual, n_classes)</span><br></pre></td></tr></table></figure></div></p>
<p>其实就是在各个输入的地方加上类别标签相关的参数。另外，别忘记在配置文件的<code>network</code>部分加一条：
<div class="code-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">network:</span></span><br><span class="line">  <span class="attr">n_classes:</span> <span class="number">10</span></span><br></pre></td></tr></table></figure></div></p>
<h2 id="自注意力机制引入">自注意力机制引入</h2>
<p>在<code>networks.py</code>中添加<code>SelfAttention</code>类，实现如下：
<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SelfAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channels: <span class="built_in">int</span>, num_heads: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.channels = channels</span><br><span class="line">        <span class="variable language_">self</span>.mha = nn.MultiheadAttention(channels, num_heads, batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.ln = nn.LayerNorm([channels])</span><br><span class="line">        <span class="variable language_">self</span>.ff_self = nn.Sequential(</span><br><span class="line">            nn.LayerNorm([channels]),</span><br><span class="line">            nn.Linear(channels, channels),</span><br><span class="line">            nn.GELU(),</span><br><span class="line">            nn.Linear(channels, channels),</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>):</span><br><span class="line">        size = x.shape[-<span class="number">2</span>:]</span><br><span class="line">        x = x.view(x.shape[<span class="number">0</span>], <span class="variable language_">self</span>.channels, -<span class="number">1</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        x_ln = <span class="variable language_">self</span>.ln(x)</span><br><span class="line">        attention_value, _ = <span class="variable language_">self</span>.mha(x_ln, x_ln, x_ln)</span><br><span class="line">        attention_value = attention_value + x</span><br><span class="line">        attention_value = <span class="variable language_">self</span>.ff_self(attention_value) + attention_value</span><br><span class="line">        <span class="keyword">return</span> attention_value.transpose(<span class="number">1</span>, <span class="number">2</span>).view(x.shape[<span class="number">0</span>], <span class="variable language_">self</span>.channels, *size)</span><br></pre></td></tr></table></figure></div></p>
<p>该自注意力模块先对传入的图片张量进行标准化，再对其作多头注意力运算，将注意力值与图片张量残差相加后用一个前馈网络对其进行进一步处理。我们将这个模块应用于UNet的<code>mid block</code>中。在<code>Unet</code>类的初始化方法参数中增加一个<code>use_attention: bool=True</code>，并将该方法中的<code>mid block</code>部分做如下改动：
<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mid block</span></span><br><span class="line"><span class="variable language_">self</span>.pe_mid = nn.Linear(pe_dim, prev_channel)</span><br><span class="line">channel = channels[-<span class="number">1</span>]</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> use_attention:</span><br><span class="line">    <span class="variable language_">self</span>.mid = nn.Sequential(</span><br><span class="line">        UNetBlock((prev_channel, Hs[-<span class="number">1</span>], Ws[-<span class="number">1</span>]),</span><br><span class="line">                  prev_channel,</span><br><span class="line">                  channel,</span><br><span class="line">                  residual=residual),</span><br><span class="line">        UNetBlock((channel, Hs[-<span class="number">1</span>], Ws[-<span class="number">1</span>]),</span><br><span class="line">                  channel,</span><br><span class="line">                  channel,</span><br><span class="line">                  residual=residual)</span><br><span class="line">    )</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="variable language_">self</span>.mid = nn.Sequential(</span><br><span class="line">        UNetBlock((prev_channel, Hs[-<span class="number">1</span>], Ws[-<span class="number">1</span>]),</span><br><span class="line">                  prev_channel,</span><br><span class="line">                  channel,</span><br><span class="line">                  residual=residual),</span><br><span class="line">        SelfAttention(channel, <span class="number">4</span>),</span><br><span class="line">        UNetBlock((channel, Hs[-<span class="number">1</span>], Ws[-<span class="number">1</span>]),</span><br><span class="line">                  channel,</span><br><span class="line">                  channel,</span><br><span class="line">                  residual=residual)</span><br><span class="line">    )</span><br><span class="line">prev_channel = channel</span><br></pre></td></tr></table></figure></div></p>
<p>相应地修改<code>build_network</code>函数的参数，并在配置文件添加：
<div class="code-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">network:</span></span><br><span class="line">  <span class="attr">attention:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></div></p>
<h2 id="学习率调整策略">学习率调整策略</h2>
<p>尝试使用余弦退火调度策略动态调整学习率。计算公式为：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/posts/ddpm/cosine_annealing_lr.png"
                      
                ></p>
<p>其中η_t为当前学习率，η_max和η_min分别为最大（初始自定）和最小学习率；T_cur为当前轮次，T_max为总轮次。学习率变化遵循余弦函数，从最大值平稳下降到最小值，下降速度在开始和结束时较慢，中间阶段较快。这有助于模型在初期快速接近最优解，中期稳定下降，后期做细微调整。</p>
<p>在<code>train.py</code>中添加函数： <div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_lr_scheduler</span>(<span class="params">optimizer</span>):</span><br><span class="line">    <span class="keyword">if</span> train_opts[<span class="string">&#x27;lr_scheduler&#x27;</span>][<span class="string">&#x27;type&#x27;</span>] == <span class="string">&#x27;cosine&#x27;</span>:</span><br><span class="line">    <span class="comment"># 配置文件中这里就可以选&#x27;MSE&#x27;或&#x27;hybrid&#x27;了</span></span><br><span class="line">        <span class="keyword">return</span> torch.optim.lr_scheduler.CosineAnnealingLR(</span><br><span class="line">            optimizer,</span><br><span class="line">            T_max=train_opts[<span class="string">&#x27;n_epochs&#x27;</span>],</span><br><span class="line">            eta_min=<span class="number">1e-6</span></span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure></div>
对应地，在<code>train()</code>中找到定义<code>optimizer</code>处，添加一行语句：
<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scheduler = get_lr_scheduler(optimizer)</span><br></pre></td></tr></table></figure></div> 在训练循环中作如下修改： <div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line"><span class="comment"># ...existing code...</span></span><br><span class="line">    <span class="keyword">for</span> x, label <span class="keyword">in</span> tqdm(dataloader, ncols=<span class="number">60</span>):</span><br><span class="line">        <span class="comment"># ...existing code...</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        scheduler.step() <span class="keyword">if</span> scheduler <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span>  <span class="comment"># added line</span></span><br></pre></td></tr></table></figure></div></p>
<p>以上就是本次的主要调整。因为重构的部分比较零碎，而上面介绍中对代码的修改难免有疏漏的地方难以排查，所以在这里记录这部分修改完成后完整的代码记录。</p>
<details class="blue" data-header-exclude><summary><i class="fa-solid fa-chevron-right"></i>`dataset.py` </summary>
              <div class='content'>
              <div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> yaml</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;options.yml&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    opt = yaml.safe_load(f)</span><br><span class="line">dataset_opts = opt[<span class="string">&#x27;dataset&#x27;</span>]</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataset</span>(<span class="params">batch_size</span>):</span><br><span class="line">    </span><br><span class="line">    trainset = <span class="literal">None</span></span><br><span class="line">    dataset_name = dataset_opts[<span class="string">&#x27;name&#x27;</span>]</span><br><span class="line">    root = dataset_opts[<span class="string">&#x27;root&#x27;</span>]</span><br><span class="line">    <span class="keyword">if</span> dataset_name == <span class="string">&#x27;mnist&#x27;</span>:</span><br><span class="line">        transform = transforms.Compose([</span><br><span class="line">            transforms.ToTensor(),</span><br><span class="line">            transforms.Normalize(mean=[<span class="number">0.5</span>], std=[<span class="number">0.5</span>])</span><br><span class="line">        ])</span><br><span class="line">        trainset = torchvision.datasets.MNIST(</span><br><span class="line">            root=root,</span><br><span class="line">            train=<span class="literal">True</span>,</span><br><span class="line">            download=<span class="literal">True</span>,</span><br><span class="line">            transform=transform</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">elif</span> dataset_name == <span class="string">&#x27;cifar10&#x27;</span>:</span><br><span class="line">        transform = transforms.Compose([</span><br><span class="line">            transforms.RandomHorizontalFlip(),</span><br><span class="line">            transforms.RandomCrop(<span class="number">32</span>, padding=<span class="number">4</span>),</span><br><span class="line">            transforms.ToTensor(),</span><br><span class="line">            transforms.Normalize(mean=[<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], std=[<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>])</span><br><span class="line">        ])</span><br><span class="line">        trainset = torchvision.datasets.CIFAR10(</span><br><span class="line">            root=root,</span><br><span class="line">            train=<span class="literal">True</span>,</span><br><span class="line">            download=<span class="literal">True</span>,</span><br><span class="line">            transform=transform</span><br><span class="line">        )</span><br><span class="line">    trainloader = DataLoader(trainset, batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> trainloader</span><br></pre></td></tr></table></figure></div>
              </div>
            </details>
<details class="blue" data-header-exclude><summary><i class="fa-solid fa-chevron-right"></i>`logger.py` </summary>
              <div class='content'>
              <div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_logger</span>(<span class="params">log_root, log_name=<span class="string">&#x27;log.log&#x27;</span></span>):</span><br><span class="line">    logger = logging.getLogger(__name__)</span><br><span class="line">    logger.setLevel(level=logging.INFO)</span><br><span class="line">    formatter = logging.Formatter(<span class="string">&#x27;%(asctime)s - %(levelname)s - %(message)s&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> os.path.isdir(log_root), <span class="string">f&quot;<span class="subst">&#123;log_root&#125;</span> is not a directory&quot;</span></span><br><span class="line">    handler = logging.FileHandler(os.path.join(log_root, log_name))</span><br><span class="line">    handler.setFormatter(formatter)</span><br><span class="line">    </span><br><span class="line">    logger.addHandler(handler)</span><br><span class="line">    <span class="keyword">return</span> logger</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dict2info</span>(<span class="params">info_name: <span class="built_in">str</span>, opt_dict: <span class="built_in">dict</span></span>):</span><br><span class="line">    info_str = <span class="string">f&#x27;<span class="subst">&#123;info_name&#125;</span>:&#x27;</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> opt_dict.keys():</span><br><span class="line">        info_str = info_str + <span class="string">f&#x27;\n\t[<span class="subst">&#123;key&#125;</span>: <span class="subst">&#123;opt_dict[key]&#125;</span>]&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> info_str</span><br></pre></td></tr></table></figure></div>
              </div>
            </details>
<details class="blue" data-header-exclude><summary><i class="fa-solid fa-chevron-right"></i>`modules/ddpm.py` </summary>
              <div class='content'>
              <div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DDPM</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 device,</span></span><br><span class="line"><span class="params">                 n_steps: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 min_beta: <span class="built_in">float</span> = <span class="number">0.0001</span>,</span></span><br><span class="line"><span class="params">                 max_beta: <span class="built_in">float</span> = <span class="number">0.02</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.n_steps = n_steps</span><br><span class="line">        <span class="variable language_">self</span>.betas = torch.linspace(min_beta, max_beta, n_steps).to(device)</span><br><span class="line">        <span class="variable language_">self</span>.alphas = <span class="number">1</span> - <span class="variable language_">self</span>.betas</span><br><span class="line">        <span class="variable language_">self</span>.alphas_bars = torch.empty_like(<span class="variable language_">self</span>.alphas)</span><br><span class="line">        product = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i, alpha <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.alphas):</span><br><span class="line">            product *= alpha</span><br><span class="line">            <span class="variable language_">self</span>.alphas_bars[i] = product</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sample_forward</span>(<span class="params">self, x, t, eps=<span class="literal">None</span></span>):</span><br><span class="line">        alpha_bar = <span class="variable language_">self</span>.alphas_bars[t].reshape(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> eps <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            eps = torch.randn_like(x)</span><br><span class="line">        res = eps * torch.sqrt(<span class="number">1</span> - alpha_bar) + torch.sqrt(alpha_bar) * x</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sample_backward</span>(<span class="params">self, img_shape, net, device, simple_var=<span class="literal">True</span>, label=<span class="literal">None</span></span>):</span><br><span class="line">        x = torch.randn(img_shape).to(device)</span><br><span class="line">        net = net.to(device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> label <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(label, torch.Tensor):</span><br><span class="line">                label = torch.tensor([label] * img_shape[<span class="number">0</span>],</span><br><span class="line">                                     dtype=torch.long).to(device)</span><br><span class="line">            label = label.reshape(-<span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># (batch_size, 1)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.n_steps-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">            x = <span class="variable language_">self</span>.sample_backward_step(x, t, net, simple_var, label)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sample_backward_step</span>(<span class="params">self, x_t, t, net, simple_var=<span class="literal">True</span>, label=<span class="literal">None</span></span>):</span><br><span class="line">        n = x_t.shape[<span class="number">0</span>]  <span class="comment"># batch size</span></span><br><span class="line">        t_tensor = torch.tensor([t] * n, dtype=torch.long).to(x_t.device).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        eps = net(x_t, t_tensor, label)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> t == <span class="number">0</span>:</span><br><span class="line">            noise = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># simple_var 用于控制取值方式</span></span><br><span class="line">            <span class="keyword">if</span> simple_var:</span><br><span class="line">                var = <span class="variable language_">self</span>.betas[t]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                var = (<span class="number">1</span>-<span class="variable language_">self</span>.alphas_bars[t-<span class="number">1</span>])/(<span class="number">1</span>-<span class="variable language_">self</span>.alphas_bars[t])*<span class="variable language_">self</span>.betas[t]</span><br><span class="line">            noise = torch.randn_like(x_t)</span><br><span class="line">            noise *= torch.sqrt(var)</span><br><span class="line"></span><br><span class="line">        mean = (x_t - (<span class="number">1</span>-<span class="variable language_">self</span>.alphas[t])/torch.sqrt(<span class="number">1</span>-<span class="variable language_">self</span>.alphas_bars[t])*eps) / \</span><br><span class="line">            torch.sqrt(<span class="variable language_">self</span>.alphas[t])</span><br><span class="line">        </span><br><span class="line">        x_t = mean + noise</span><br><span class="line">        <span class="keyword">return</span> x_t  <span class="comment"># updated image</span></span><br><span class="line">        </span><br></pre></td></tr></table></figure></div>
              </div>
            </details>
<details class="blue" data-header-exclude><summary><i class="fa-solid fa-chevron-right"></i>`modules/networks.py` </summary>
              <div class='content'>
              <div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> yaml</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;options.yml&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    opt = yaml.safe_load(f)</span><br><span class="line">img_shape = opt[<span class="string">&#x27;dataset&#x27;</span>][<span class="string">&#x27;img_shape&#x27;</span>][opt[<span class="string">&#x27;dataset&#x27;</span>][<span class="string">&#x27;name&#x27;</span>]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                 max_seq_len: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 d_model: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 n_classes: <span class="built_in">int</span>=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Assume d_model is an even number for convenience</span></span><br><span class="line">        <span class="keyword">assert</span> d_model % <span class="number">2</span> == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># time step encoding</span></span><br><span class="line">        pe = torch.zeros(max_seq_len, d_model)</span><br><span class="line">        i_seq = torch.linspace(<span class="number">0</span>, max_seq_len - <span class="number">1</span>, max_seq_len)</span><br><span class="line">        j_seq = torch.linspace(<span class="number">0</span>, d_model - <span class="number">2</span>, d_model // <span class="number">2</span>)</span><br><span class="line">        pos, two_i = torch.meshgrid(i_seq, j_seq)</span><br><span class="line">        pe_2i = torch.sin(pos / <span class="number">1e4</span> ** (two_i / d_model))</span><br><span class="line">        pe_2i_1 = torch.cos(pos / <span class="number">1e4</span> ** (two_i / d_model))</span><br><span class="line">        pe = torch.stack((pe_2i, pe_2i_1), <span class="number">2</span>).reshape(max_seq_len, d_model)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.t_embedding = nn.Embedding(max_seq_len, d_model)</span><br><span class="line">        <span class="variable language_">self</span>.t_embedding.weight.data = pe</span><br><span class="line">        <span class="variable language_">self</span>.t_embedding.requires_grad_(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># label encoding</span></span><br><span class="line">        <span class="variable language_">self</span>.use_condition = n_classes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_condition:</span><br><span class="line">            <span class="variable language_">self</span>.label_embedding = nn.Embedding(n_classes, d_model)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, t, label=<span class="literal">None</span></span>):</span><br><span class="line">        t_emb = <span class="variable language_">self</span>.t_embedding(t)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_condition <span class="keyword">and</span> label <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            label_emb = <span class="variable language_">self</span>.label_embedding(label)</span><br><span class="line">            <span class="keyword">return</span> t_emb + label_emb</span><br><span class="line">        <span class="keyword">return</span> t_emb</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SelfAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channels: <span class="built_in">int</span>, num_heads: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.channels = channels</span><br><span class="line">        <span class="variable language_">self</span>.mha = nn.MultiheadAttention(channels, num_heads, batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.ln = nn.LayerNorm([channels])</span><br><span class="line">        <span class="variable language_">self</span>.ff_self = nn.Sequential(</span><br><span class="line">            nn.LayerNorm([channels]),</span><br><span class="line">            nn.Linear(channels, channels),</span><br><span class="line">            nn.GELU(),</span><br><span class="line">            nn.Linear(channels, channels),</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>):</span><br><span class="line">        size = x.shape[-<span class="number">2</span>:]</span><br><span class="line">        x = x.view(x.shape[<span class="number">0</span>], <span class="variable language_">self</span>.channels, -<span class="number">1</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        x_ln = <span class="variable language_">self</span>.ln(x)</span><br><span class="line">        attention_value, _ = <span class="variable language_">self</span>.mha(x_ln, x_ln, x_ln)</span><br><span class="line">        attention_value = attention_value + x</span><br><span class="line">        attention_value = <span class="variable language_">self</span>.ff_self(attention_value) + attention_value</span><br><span class="line">        <span class="keyword">return</span> attention_value.transpose(<span class="number">1</span>, <span class="number">2</span>).view(x.shape[<span class="number">0</span>], <span class="variable language_">self</span>.channels, *size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">UNetBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 shape,</span></span><br><span class="line"><span class="params">                 in_c,</span></span><br><span class="line"><span class="params">                 out_c,</span></span><br><span class="line"><span class="params">                 residual=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.ln = nn.LayerNorm(shape)</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(in_c, out_c, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(out_c, out_c, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.activation = nn.ReLU()</span><br><span class="line">        <span class="variable language_">self</span>.residual = residual</span><br><span class="line">        <span class="keyword">if</span> residual:</span><br><span class="line">            <span class="keyword">if</span> in_c == out_c:</span><br><span class="line">                <span class="variable language_">self</span>.residual_conv = nn.Identity()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="variable language_">self</span>.residual_conv = nn.Conv2d(in_c, out_c, kernel_size=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = <span class="variable language_">self</span>.activation(<span class="variable language_">self</span>.conv1(<span class="variable language_">self</span>.ln(x)))</span><br><span class="line">        out = <span class="variable language_">self</span>.conv2(out)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.residual:</span><br><span class="line">            out += <span class="variable language_">self</span>.residual_conv(x)</span><br><span class="line">        out = <span class="variable language_">self</span>.activation(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">UNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                 n_steps: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 channels: <span class="built_in">list</span>=[<span class="number">10</span>, <span class="number">20</span>, <span class="number">40</span>, <span class="number">80</span>],</span></span><br><span class="line"><span class="params">                 pe_dim: <span class="built_in">int</span>=<span class="number">10</span>,</span></span><br><span class="line"><span class="params">                 residual: <span class="built_in">bool</span>=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 n_classes: <span class="built_in">int</span>=<span class="number">10</span>, </span></span><br><span class="line"><span class="params">                 use_attention: <span class="built_in">bool</span> = <span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        C, H, W = img_shape[<span class="number">0</span>], img_shape[<span class="number">1</span>], img_shape[<span class="number">2</span>]</span><br><span class="line">        n_layers = <span class="built_in">len</span>(channels)</span><br><span class="line">        Hs = [H]</span><br><span class="line">        Ws = [W]</span><br><span class="line">        cH = H</span><br><span class="line">        cW = W</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers - <span class="number">1</span>):</span><br><span class="line">            cH //= <span class="number">2</span></span><br><span class="line">            cW //= <span class="number">2</span></span><br><span class="line">            Hs.append(cH)</span><br><span class="line">            Ws.append(cW)</span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.pe = PositionalEncoding(n_steps, pe_dim, n_classes)</span><br><span class="line">        <span class="variable language_">self</span>.encoders = nn.ModuleList()</span><br><span class="line">        <span class="variable language_">self</span>.decoders = nn.ModuleList()</span><br><span class="line">        <span class="variable language_">self</span>.pe_linears_en = nn.ModuleList()</span><br><span class="line">        <span class="variable language_">self</span>.pe_linears_de = nn.ModuleList()</span><br><span class="line">        <span class="variable language_">self</span>.downs = nn.ModuleList()</span><br><span class="line">        <span class="variable language_">self</span>.ups = nn.ModuleList()</span><br><span class="line">        prev_channel = C</span><br><span class="line"></span><br><span class="line">        <span class="comment"># down blocks</span></span><br><span class="line">        <span class="keyword">for</span> channel, cH, cW <span class="keyword">in</span> <span class="built_in">zip</span>(channels[<span class="number">0</span>:-<span class="number">1</span>], Hs[<span class="number">0</span>:-<span class="number">1</span>], Ws[<span class="number">0</span>:-<span class="number">1</span>]):</span><br><span class="line">            <span class="variable language_">self</span>.pe_linears_en.append(</span><br><span class="line">                nn.Sequential(nn.Linear(pe_dim, prev_channel),</span><br><span class="line">                              nn.ReLU(),</span><br><span class="line">                              nn.Linear(prev_channel, prev_channel))</span><br><span class="line">            )</span><br><span class="line">            <span class="variable language_">self</span>.encoders.append(</span><br><span class="line">                nn.Sequential(</span><br><span class="line">                    UNetBlock((prev_channel, cH, cW),</span><br><span class="line">                              prev_channel,</span><br><span class="line">                              channel,</span><br><span class="line">                              residual=residual),</span><br><span class="line">                    UNetBlock((channel, cH, cW),</span><br><span class="line">                              channel,</span><br><span class="line">                              channel,</span><br><span class="line">                              residual=residual)</span><br><span class="line">                )</span><br><span class="line">            )</span><br><span class="line">            <span class="variable language_">self</span>.downs.append(nn.Conv2d(channel, channel, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line">            prev_channel = channel</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># mid block</span></span><br><span class="line">        <span class="variable language_">self</span>.pe_mid = nn.Linear(pe_dim, prev_channel)</span><br><span class="line">        channel = channels[-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> use_attention:</span><br><span class="line">            <span class="variable language_">self</span>.mid = nn.Sequential(</span><br><span class="line">                UNetBlock((prev_channel, Hs[-<span class="number">1</span>], Ws[-<span class="number">1</span>]),</span><br><span class="line">                          prev_channel,</span><br><span class="line">                          channel,</span><br><span class="line">                          residual=residual),</span><br><span class="line">                UNetBlock((channel, Hs[-<span class="number">1</span>], Ws[-<span class="number">1</span>]),</span><br><span class="line">                          channel,</span><br><span class="line">                          channel,</span><br><span class="line">                          residual=residual)</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.mid = nn.Sequential(</span><br><span class="line">                UNetBlock((prev_channel, Hs[-<span class="number">1</span>], Ws[-<span class="number">1</span>]),</span><br><span class="line">                          prev_channel,</span><br><span class="line">                          channel,</span><br><span class="line">                          residual=residual),</span><br><span class="line">                SelfAttention(channel, <span class="number">4</span>),</span><br><span class="line">                UNetBlock((channel, Hs[-<span class="number">1</span>], Ws[-<span class="number">1</span>]),</span><br><span class="line">                          channel,</span><br><span class="line">                          channel,</span><br><span class="line">                          residual=residual)</span><br><span class="line">            )</span><br><span class="line">        prev_channel = channel</span><br><span class="line"></span><br><span class="line">        <span class="comment"># up blocks</span></span><br><span class="line">        <span class="keyword">for</span> channel, cH, cW <span class="keyword">in</span> <span class="built_in">zip</span>(channels[-<span class="number">2</span>::-<span class="number">1</span>], Hs[-<span class="number">2</span>::-<span class="number">1</span>], Ws[-<span class="number">2</span>::-<span class="number">1</span>]):</span><br><span class="line">            <span class="variable language_">self</span>.pe_linears_de.append(nn.Linear(pe_dim, prev_channel))</span><br><span class="line">            <span class="variable language_">self</span>.decoders.append(</span><br><span class="line">                nn.Sequential(</span><br><span class="line">                    UNetBlock((channel * <span class="number">2</span>, cH, cW),</span><br><span class="line">                              channel * <span class="number">2</span>,</span><br><span class="line">                              channel,</span><br><span class="line">                              residual=residual),</span><br><span class="line">                    UNetBlock((channel, cH, cW),</span><br><span class="line">                              channel,</span><br><span class="line">                              channel,</span><br><span class="line">                              residual=residual)</span><br><span class="line">                )</span><br><span class="line">            )</span><br><span class="line">            <span class="variable language_">self</span>.ups.append(nn.ConvTranspose2d(prev_channel, channel, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line">            prev_channel = channel</span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.conv_out = nn.Conv2d(prev_channel, C, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, t, label=<span class="literal">None</span></span>):</span><br><span class="line">        n = t.shape[<span class="number">0</span>]</span><br><span class="line">        t = <span class="variable language_">self</span>.pe(t, label)</span><br><span class="line">        encoder_outs = []</span><br><span class="line">        <span class="keyword">for</span> pe_linear, encoder, down <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.pe_linears_en, <span class="variable language_">self</span>.encoders, <span class="variable language_">self</span>.downs):</span><br><span class="line">            pe = pe_linear(t).reshape(n, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            x = encoder(x + pe)</span><br><span class="line">            encoder_outs.append(x)</span><br><span class="line">            x = down(x)</span><br><span class="line">        pe = <span class="variable language_">self</span>.pe_mid(t).reshape(n, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        x = <span class="variable language_">self</span>.mid(x + pe)</span><br><span class="line">        <span class="keyword">for</span> pe_linear, decoder, up, encoder_out <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.pe_linears_de, <span class="variable language_">self</span>.decoders, </span><br><span class="line">                                                        <span class="variable language_">self</span>.ups, encoder_outs[::-<span class="number">1</span>]):</span><br><span class="line">            pe = pe_linear(t).reshape(n, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            x = up(x)</span><br><span class="line">            pad_x = encoder_out.shape[<span class="number">2</span>] - x.shape[<span class="number">2</span>]</span><br><span class="line">            pad_y = encoder_out.shape[<span class="number">3</span>] - x.shape[<span class="number">3</span>]</span><br><span class="line">            x = F.pad(x,</span><br><span class="line">                      (pad_x // <span class="number">2</span>, pad_x - pad_x//<span class="number">2</span>, pad_y // <span class="number">2</span>, pad_y - pad_y//<span class="number">2</span>))</span><br><span class="line">            x = torch.cat((encoder_out, x), dim=<span class="number">1</span>)</span><br><span class="line">            x = decoder(x + pe)</span><br><span class="line">        x = <span class="variable language_">self</span>.conv_out(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_network</span>(<span class="params">n_steps: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">                  channels: <span class="built_in">list</span>, </span></span><br><span class="line"><span class="params">                  pe_dim: <span class="built_in">bool</span>=<span class="literal">None</span>, </span></span><br><span class="line"><span class="params">                  residual: <span class="built_in">bool</span>=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                  n_classes: <span class="built_in">int</span>=<span class="number">10</span>,</span></span><br><span class="line"><span class="params">                  use_attention: <span class="built_in">bool</span>=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="keyword">return</span> UNet(n_steps, channels, pe_dim, residual, n_classes, use_attention)</span><br><span class="line"></span><br></pre></td></tr></table></figure></div>
              </div>
            </details>
<details class="blue" data-header-exclude><summary><i class="fa-solid fa-chevron-right"></i>`train.py` </summary>
              <div class='content'>
              <div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> dataset <span class="keyword">import</span> get_dataset</span><br><span class="line"><span class="keyword">from</span> modules.networks <span class="keyword">import</span> build_network</span><br><span class="line"><span class="keyword">from</span> modules.ddpm <span class="keyword">import</span> DDPM</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorboardX <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> yaml</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> logger <span class="keyword">import</span> get_logger, dict2info</span><br><span class="line"></span><br><span class="line"><span class="comment">## initialization</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;options.yml&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    opt = yaml.safe_load(f)</span><br><span class="line">    f.close()</span><br><span class="line">curr_time = datetime.now().strftime(<span class="string">&quot;%Y%m%d-%H%M%S&quot;</span>)</span><br><span class="line">save_root = <span class="string">f&#x27;models/<span class="subst">&#123;curr_time&#125;</span>-<span class="subst">&#123;opt[<span class="string">&#x27;dataset&#x27;</span>][<span class="string">&#x27;name&#x27;</span>]&#125;</span>&#x27;</span></span><br><span class="line">os.makedirs(os.path.join(save_root, <span class="string">&#x27;ckpts&#x27;</span>))</span><br><span class="line">logger = get_logger(save_root, <span class="string">&#x27;train.log&#x27;</span>)</span><br><span class="line">train_opts = opt[<span class="string">&#x27;train&#x27;</span>]</span><br><span class="line">writer = SummaryWriter()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_lr_scheduler</span>(<span class="params">optimizer</span>):</span><br><span class="line">    <span class="keyword">if</span> train_opts[<span class="string">&#x27;lr_scheduler&#x27;</span>][<span class="string">&#x27;type&#x27;</span>] == <span class="string">&#x27;cosine&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.optim.lr_scheduler.CosineAnnealingLR(</span><br><span class="line">            optimizer,</span><br><span class="line">            T_max=train_opts[<span class="string">&#x27;n_epochs&#x27;</span>],</span><br><span class="line">            eta_min=<span class="number">1e-6</span></span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">ddpm: DDPM, dataloader, net: nn.Module, device</span>):</span><br><span class="line">    n_epochs = train_opts[<span class="string">&#x27;n_epochs&#x27;</span>]</span><br><span class="line">    n_steps = ddpm.n_steps</span><br><span class="line">    net = net.to(device)</span><br><span class="line">    <span class="keyword">if</span> train_opts[<span class="string">&#x27;loss&#x27;</span>] == <span class="string">&#x27;MSE&#x27;</span>:</span><br><span class="line">        loss_fn = nn.MSELoss()</span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> more loss functions</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">f&quot;Unknown loss function: <span class="subst">&#123;train_opts[<span class="string">&#x27;loss&#x27;</span>]&#125;</span>&quot;</span>)</span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(), <span class="built_in">float</span>(train_opts[<span class="string">&#x27;lr&#x27;</span>]))</span><br><span class="line">    scheduler = get_lr_scheduler(optimizer)</span><br><span class="line"></span><br><span class="line">    logger.info(<span class="string">&quot;Start training.&quot;</span>)</span><br><span class="line">    step = <span class="number">0</span></span><br><span class="line">    resume_epochs_passed = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    ckpt_path: <span class="built_in">str</span> = train_opts[<span class="string">&#x27;resume&#x27;</span>]</span><br><span class="line">    <span class="keyword">if</span> ckpt_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">if</span> os.path.exists(ckpt_path):</span><br><span class="line">            net.load_state_dict(torch.load(ckpt_path))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;Load model from <span class="subst">&#123;ckpt_path&#125;</span>.&#x27;</span>)</span><br><span class="line">            logger.info(<span class="string">f&#x27;Load model from <span class="subst">&#123;ckpt_path&#125;</span>.&#x27;</span>)</span><br><span class="line">            resume_epochs_passed = <span class="built_in">int</span>(ckpt_path.split(<span class="string">&#x27;/&#x27;</span>)[-<span class="number">1</span>].split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>].split(<span class="string">&#x27;_&#x27;</span>)[-<span class="number">1</span>]) + <span class="number">1</span></span><br><span class="line">            step += resume_epochs_passed * <span class="built_in">len</span>(dataloader)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">        total_loss = <span class="number">0</span></span><br><span class="line">        truth_epoch = epoch + resume_epochs_passed</span><br><span class="line">        <span class="keyword">for</span> x, label <span class="keyword">in</span> tqdm(dataloader, ncols=<span class="number">60</span>):</span><br><span class="line">            batch_size = x.shape[<span class="number">0</span>]</span><br><span class="line">            x, label = x.to(device), label.to(device)</span><br><span class="line">            </span><br><span class="line">            t = torch.randint(<span class="number">0</span>, n_steps, (batch_size, )).to(device)</span><br><span class="line">            eps = torch.randn_like(x).to(device)</span><br><span class="line">            x_t = ddpm.sample_forward(x, t, eps)</span><br><span class="line">            eps_theta = net(x_t, t.reshape(batch_size, <span class="number">1</span>), label.reshape(batch_size, <span class="number">1</span>))</span><br><span class="line">            loss = loss_fn(eps_theta, eps)</span><br><span class="line">            writer.add_scalar(<span class="string">&#x27;loss/step&#x27;</span>, loss, step)</span><br><span class="line">            total_loss += loss</span><br><span class="line">            step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            scheduler.step() <span class="keyword">if</span> scheduler <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        epoch_loss = total_loss / <span class="built_in">len</span>(dataloader)</span><br><span class="line">        logger.info(<span class="string">f&quot;epoch <span class="subst">&#123;truth_epoch&#125;</span> | loss <span class="subst">&#123;epoch_loss&#125;</span>&quot;</span>)</span><br><span class="line">        writer.add_scalar(<span class="string">&#x27;loss/epoch&#x27;</span>, epoch_loss, truth_epoch)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;truth_epoch&#125;</span> | loss <span class="subst">&#123;epoch_loss&#125;</span>&#x27;</span>)</span><br><span class="line">        save_path = os.path.join(save_root, <span class="string">&#x27;ckpts&#x27;</span>, <span class="string">f&#x27;epoch_<span class="subst">&#123;truth_epoch&#125;</span>.pth&#x27;</span>)</span><br><span class="line">        torch.save(net.state_dict(), save_path)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Model checkpoint has been saved into <span class="subst">&#123;save_path&#125;</span>.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    logger.info(<span class="string">&quot;Training stage finished.&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># get dataloader</span></span><br><span class="line">    dataloader = get_dataset(batch_size=train_opts[<span class="string">&#x27;batch_size&#x27;</span>])</span><br><span class="line">    logger.info(<span class="string">f&#x27;dataset: <span class="subst">&#123;opt[<span class="string">&#x27;dataset&#x27;</span>][<span class="string">&#x27;name&#x27;</span>]&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># set device</span></span><br><span class="line">    device = opt[<span class="string">&#x27;device&#x27;</span>]</span><br><span class="line">    logger.info(<span class="string">f&#x27;device: <span class="subst">&#123;device&#125;</span>&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># DDPM settings</span></span><br><span class="line">    ddpm_opts = opt[<span class="string">&#x27;ddpm&#x27;</span>]</span><br><span class="line">    ddpm_info = dict2info(<span class="string">&quot;DDPM&quot;</span>, <span class="built_in">dict</span>(ddpm_opts))</span><br><span class="line">    logger.info(ddpm_info)</span><br><span class="line">    ddpm = DDPM(device, </span><br><span class="line">                ddpm_opts[<span class="string">&#x27;n_steps&#x27;</span>], </span><br><span class="line">                <span class="built_in">float</span>(ddpm_opts[<span class="string">&#x27;min_beta&#x27;</span>]), </span><br><span class="line">                <span class="built_in">float</span>(ddpm_opts[<span class="string">&#x27;max_beta&#x27;</span>])</span><br><span class="line">            )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># network settings</span></span><br><span class="line">    network_opts = opt[<span class="string">&#x27;network&#x27;</span>]</span><br><span class="line">    net_info = dict2info(<span class="string">&quot;network&quot;</span>, <span class="built_in">dict</span>(network_opts))</span><br><span class="line">    logger.info(net_info)</span><br><span class="line">    net = build_network(n_steps=ddpm_opts[<span class="string">&#x27;n_steps&#x27;</span>],</span><br><span class="line">                        channels=network_opts[<span class="string">&#x27;channels&#x27;</span>],</span><br><span class="line">                        pe_dim=network_opts[<span class="string">&#x27;pe_dim&#x27;</span>],</span><br><span class="line">                        residual=network_opts[<span class="string">&#x27;residual&#x27;</span>],</span><br><span class="line">                        n_classes=network_opts[<span class="string">&#x27;n_classes&#x27;</span>],</span><br><span class="line">                        use_attention=network_opts[<span class="string">&#x27;attention&#x27;</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># start training</span></span><br><span class="line">    train_info = dict2info(<span class="string">&quot;training options&quot;</span>, <span class="built_in">dict</span>(train_opts))</span><br><span class="line">    logger.info(train_info)</span><br><span class="line">    train(ddpm, dataloader, net, device)</span><br><span class="line"></span><br></pre></td></tr></table></figure></div>
              </div>
            </details>
<details class="blue" data-header-exclude><summary><i class="fa-solid fa-chevron-right"></i>`test.py` </summary>
              <div class='content'>
              <div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> einops</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> yaml</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> modules.ddpm <span class="keyword">import</span> DDPM</span><br><span class="line"><span class="keyword">from</span> modules.networks <span class="keyword">import</span> build_network</span><br><span class="line"><span class="keyword">from</span> logger <span class="keyword">import</span> get_logger, dict2info</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line">curr_time = datetime.now().strftime(<span class="string">&quot;%Y%m%d-%H%M%S&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;options.yml&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    opt = yaml.safe_load(f)</span><br><span class="line">    f.close()</span><br><span class="line">test_opts = opt[<span class="string">&#x27;test&#x27;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">ddpm: DDPM,</span></span><br><span class="line"><span class="params">             net: nn.Module,</span></span><br><span class="line"><span class="params">             output_path: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">             n_sample: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">             device,</span></span><br><span class="line"><span class="params">             img_shape,</span></span><br><span class="line"><span class="params">             simple_var=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">             label=<span class="literal">None</span></span>):</span><br><span class="line">    net = net.to(device)</span><br><span class="line">    net = net.<span class="built_in">eval</span>()</span><br><span class="line">    C, H, W = img_shape[<span class="number">0</span>], img_shape[<span class="number">1</span>], img_shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        shape = (n_sample, C, H, W)</span><br><span class="line">        imgs = ddpm.sample_backward(shape,</span><br><span class="line">                                    net,</span><br><span class="line">                                    device,</span><br><span class="line">                                    simple_var,</span><br><span class="line">                                    label).detach().to(device)</span><br><span class="line">        imgs = (imgs + <span class="number">1</span>) / <span class="number">2</span> * <span class="number">255</span></span><br><span class="line">        imgs = imgs.clamp(<span class="number">0</span>, <span class="number">255</span>)</span><br><span class="line">        imgs = einops.rearrange(imgs,</span><br><span class="line">                                <span class="string">&#x27;(b1 b2) c h w -&gt; (b1 h) (b2 w) c&#x27;</span>,</span><br><span class="line">                                b1=<span class="built_in">int</span>(n_sample**<span class="number">0.5</span>))</span><br><span class="line">        imgs = imgs.cpu()</span><br><span class="line">        imgs = imgs.numpy().astype(np.uint8)</span><br><span class="line">        cv2.imwrite(output_path, imgs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># DDPM settings</span></span><br><span class="line">    ddpm_opts = opt[<span class="string">&#x27;ddpm&#x27;</span>]</span><br><span class="line">    ddpm = DDPM(opt[<span class="string">&#x27;device&#x27;</span>], </span><br><span class="line">                <span class="built_in">int</span>(ddpm_opts[<span class="string">&#x27;n_steps&#x27;</span>]), </span><br><span class="line">                <span class="built_in">float</span>(ddpm_opts[<span class="string">&#x27;min_beta&#x27;</span>]), </span><br><span class="line">                <span class="built_in">float</span>(ddpm_opts[<span class="string">&#x27;max_beta&#x27;</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># network settings</span></span><br><span class="line">    network_opts = opt[<span class="string">&#x27;network&#x27;</span>]</span><br><span class="line">    network_opts_dict = <span class="built_in">dict</span>(network_opts)</span><br><span class="line">    net = build_network(n_steps=ddpm_opts[<span class="string">&#x27;n_steps&#x27;</span>],</span><br><span class="line">                        channels=network_opts[<span class="string">&#x27;channels&#x27;</span>],</span><br><span class="line">                        pe_dim=network_opts[<span class="string">&#x27;pe_dim&#x27;</span>],</span><br><span class="line">                        residual=network_opts[<span class="string">&#x27;residual&#x27;</span>])</span><br><span class="line">    <span class="comment"># load network checkpoint</span></span><br><span class="line">    ckpt_path = test_opts[<span class="string">&#x27;ckpt_path&#x27;</span>]</span><br><span class="line">    <span class="keyword">assert</span> os.path.exists(ckpt_path), <span class="string">f&#x27;<span class="subst">&#123;ckpt_path&#125;</span> is an invalid path.&#x27;</span></span><br><span class="line">    net.load_state_dict(torch.load(ckpt_path))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># output path settings</span></span><br><span class="line">    output_dir = os.path.join(test_opts[<span class="string">&#x27;output_dir&#x27;</span>], <span class="string">f&#x27;<span class="subst">&#123;curr_time&#125;</span>&#x27;</span>)</span><br><span class="line">    os.makedirs(output_dir) <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(output_dir) <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># logger settings</span></span><br><span class="line">    logger = get_logger(output_dir, <span class="string">&#x27;test.log&#x27;</span>)</span><br><span class="line">    logger.info(dict2info(<span class="string">&quot;DDPM&quot;</span>, <span class="built_in">dict</span>(ddpm_opts)))</span><br><span class="line">    logger.info(dict2info(<span class="string">&quot;network&quot;</span>, <span class="built_in">dict</span>(network_opts)))</span><br><span class="line">    logger.info(dict2info(<span class="string">&quot;test options&quot;</span>, <span class="built_in">dict</span>(test_opts)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># other settings</span></span><br><span class="line">    n_sample = test_opts[<span class="string">&#x27;n_samples&#x27;</span>]</span><br><span class="line">    device = opt[<span class="string">&#x27;device&#x27;</span>]</span><br><span class="line">    img_shape = opt[<span class="string">&#x27;dataset&#x27;</span>][<span class="string">&#x27;img_shape&#x27;</span>][opt[<span class="string">&#x27;dataset&#x27;</span>][<span class="string">&#x27;name&#x27;</span>]]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># sample images from noise</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Output Directory: <span class="subst">&#123;output_dir&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Classes Tested: <span class="subst">&#123;test_opts[<span class="string">&#x27;classes&#x27;</span>]&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> label <span class="keyword">in</span> tqdm(test_opts[<span class="string">&#x27;classes&#x27;</span>], ncols=<span class="number">60</span>):</span><br><span class="line">        output_path = os.path.join(output_dir, <span class="string">f&#x27;<span class="subst">&#123;label&#125;</span>.png&#x27;</span>)</span><br><span class="line">        generate(ddpm=ddpm,</span><br><span class="line">                 net=net,</span><br><span class="line">                 output_path=output_path,</span><br><span class="line">                 n_sample=n_sample,</span><br><span class="line">                 device=device,</span><br><span class="line">                 img_shape=img_shape,</span><br><span class="line">                 simple_var=<span class="literal">True</span>,</span><br><span class="line">                 label=label)</span><br><span class="line">        logger.info(<span class="string">f&quot;Save result to <span class="subst">&#123;output_path&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></div>
              </div>
            </details>
<details class="purple" data-header-exclude><summary><i class="fa-solid fa-chevron-right"></i>`options.yml` </summary>
              <div class='content'>
              <div class="code-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">device:</span> <span class="string">&#x27;cuda:1&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">dataset:</span> </span><br><span class="line">  <span class="attr">name:</span> <span class="string">&#x27;cifar10&#x27;</span></span><br><span class="line">  <span class="attr">root:</span> <span class="string">&#x27;./cache&#x27;</span></span><br><span class="line">  <span class="attr">img_shape:</span></span><br><span class="line">    <span class="attr">mnist:</span> [<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>]</span><br><span class="line">    <span class="attr">cifar10:</span> [<span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>]</span><br><span class="line"></span><br><span class="line"><span class="attr">train:</span></span><br><span class="line">  <span class="attr">n_epochs:</span> <span class="number">50</span></span><br><span class="line">  <span class="attr">batch_size:</span> <span class="number">64</span></span><br><span class="line">  <span class="attr">loss:</span> <span class="string">&#x27;MSE&#x27;</span></span><br><span class="line">  <span class="attr">lr:</span> <span class="number">2e-4</span></span><br><span class="line">  <span class="attr">lr_scheduler:</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">&#x27;cosine&#x27;</span></span><br><span class="line">    <span class="attr">warmup_epochs:</span> <span class="number">5</span></span><br><span class="line">  <span class="attr">resume:</span> <span class="string">~</span></span><br><span class="line"></span><br><span class="line"><span class="attr">test:</span></span><br><span class="line">  <span class="attr">output_dir:</span> <span class="string">&#x27;./results&#x27;</span></span><br><span class="line">  <span class="attr">ckpt_path:</span> <span class="string">~</span></span><br><span class="line">  <span class="attr">n_samples:</span> <span class="number">81</span></span><br><span class="line">  <span class="attr">classes:</span> [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]</span><br><span class="line"></span><br><span class="line"><span class="attr">network:</span></span><br><span class="line">  <span class="attr">channels:</span> [<span class="number">64</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>, <span class="number">1024</span>]</span><br><span class="line">  <span class="attr">pe_dim:</span> <span class="number">256</span></span><br><span class="line">  <span class="attr">residual:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">attention:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">n_classes:</span> <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="attr">ddpm:</span></span><br><span class="line">  <span class="attr">n_steps:</span> <span class="number">1000</span></span><br><span class="line">  <span class="attr">min_beta:</span> <span class="number">1e-4</span></span><br><span class="line">  <span class="attr">max_beta:</span> <span class="number">2e-2</span></span><br></pre></td></tr></table></figure></div>
              </div>
            </details>
<h2 id="结果展示与分析">结果展示与分析</h2>
<p>首先看看改进后的模型在MNIST数据集上的生成结果（部分）：</p>
<figure>
<figure class="image-caption"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/posts/ddpm/mnist-proj2.png"
                     
alt="训练于MNIST数据集的模型在标签0，6，9条件下的生成结果" 
                ><figcaption>训练于MNIST数据集的模型在标签0，6，9条件下的生成结果</figcaption></figure>
<figcaption
aria-hidden="true">训练于MNIST数据集的模型在标签0，6，9条件下的生成结果</figcaption>
</figure>
<p>可以看到，这种标签特征融合的方式还是非常有效果的，模型针对不同类别特征顺利地生成了明显可辨识的阿拉伯数字，准确率还算不错。接下来看看CIFAR-10数据集的生成结果（部分）：</p>
<figure>
<figure class="image-caption"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/posts/ddpm/mnist-proj2.png"
                     
alt="训练于CIFAR-10数据集的模型在标签0（飞机），6（青蛙），9（卡车）条件下的生成结果" 
                ><figcaption>训练于CIFAR-10数据集的模型在标签0（飞机），6（青蛙），9（卡车）条件下的生成结果</figcaption></figure>
<figcaption
aria-hidden="true">训练于CIFAR-10数据集的模型在标签0（飞机），6（青蛙），9（卡车）条件下的生成结果</figcaption>
</figure>
<p>这个结果是模型在训练20个周期后生成的，之所以图片有明显的黑框是因为处理该数据集时使用了<code>transforms.RandomCrop(32, padding=4)</code>。与上次的色块不同，这次能明显看出来模型已经在试图生成具有现实语义的图案了，但对应的类别特征还十分不明显，基本无法分辨。如果我们尝试继续训练一段时间然后看看效果呢？</p>
<figure>
<figure class="image-caption"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/posts/ddpm/cifar10-proj2-overfitted.png"
                     
alt="过拟合（疑似？）的训练于CIFAR-10上的模型" 
                ><figcaption>过拟合（疑似？）的训练于CIFAR-10上的模型</figcaption></figure>
<figcaption
aria-hidden="true">过拟合（疑似？）的训练于CIFAR-10上的模型</figcaption>
</figure>
<p>很不幸，我们并不能通过简单延长训练阶段来提升模型效果。模型对噪声的预测逐渐发生了偏移，因此集中产生了纯黑或纯白的图像（集中生成纯黑或纯白图像的现象不会出现在同一个模型中，即一个模型的预测发生较大偏移时，要么生成纯黑图像要么生成纯白图像）。为什么会产生这种现象？</p>
<p>我的分析是：网络结构还不够强大，现有架构缺乏更妥善的注意力机制处理、更深的网络深度和针对RGB图像的特殊处理；对数据集进行适当的增强和准确的归一化也很重要（我当时简单地将数据集标准化的均值和方差设为0.5，但实际上要根据数据集本身的相关属性进行设定）。</p>
<p>下一步的改进计划是：</p>
<ol type="1">
<li><p>改良UNet网络结构</p></li>
<li><p>引入EMA方法</p></li>
<li><p>改进损失函数的衡量</p></li>
</ol>

  <div class="note p-4 mb-4 rounded-small yellow">
    <p>事实上，由于该项目是我用来熟悉项目构建流程和相关方法的，所以很多改进策略并没有相关论文材料支撑，而是主要来自我的“一拍脑袋”和LLM的建议。所以我的改进并非都有效，事实上在实验过程中也确实发生负改进的情况（模型复杂度增加了但效果并没有变好）。所以我的方法仅供参考！该系列博客只是记录我的该demo构建历程，因此我将其放在【学习笔记】的类别而非【经验分享】。希望我的思考能够帮到读者！</p>

  </div>
<p>该系列未完待续！</p>

		</div>

		
		<div class="post-copyright-info w-full my-8 px-2 sm:px-6 md:px-8">
			<div class="article-copyright-info-container">
    <ul>
        <li><strong>Title:</strong> 深度学习基础：基于DDPM模型的生成模型demo笔记（二）</li>
        <li><strong>Author:</strong> Jachin Zhang</li>
        <li><strong>Created at
                :</strong> 2025-03-06 22:26:07</li>
        
            <li>
                <strong>Updated at
                    :</strong> 2025-03-07 22:07:17
            </li>
        
        <li>
            <strong>Link:</strong> https://jachinzhang1.github.io/2025/03/06/ddpm-project-2/
        </li>
        <li>
            <strong>
                License:
            </strong>
            

            
                This work is licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0">CC BY-NC-SA 4.0</a>.
            
        </li>
    </ul>
</div>

		</div>
		

		
		<ul class="post-tags-box text-lg mt-1.5 flex-wrap justify-center flex md:hidden">
			
			<li class="tag-item mx-0.5">
				<a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">#深度学习</a>&nbsp;
			</li>
			
			<li class="tag-item mx-0.5">
				<a href="/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">#图像生成</a>&nbsp;
			</li>
			
		</ul>
		

		

		
		<div class="article-nav my-8 flex justify-between items-center px-2 sm:px-6 md:px-8">
			
			
			<div class="article-next border-border-color shadow-redefine-flat shadow-shadow-color-2 rounded-medium px-4 py-2 hover:shadow-redefine-flat-hover hover:shadow-shadow-color-2">
				<a class="next" rel="next" href="/2025/03/05/ddpm-project-1/">
					<span class="title flex justify-center items-center">
						<span class="post-nav-title-item">深度学习基础：基于DDPM模型的生成模型demo笔记（一）</span>
						<span class="post-nav-item">Next posts</span>
					</span>
					<span class="right arrow-icon flex justify-center items-center">
						<i class="fa-solid fa-chevron-right"></i>
					</span>
				</a>
			</div>
			
		</div>
		


		
		<div class="comment-container px-2 sm:px-6 md:px-8 pb-8">
			<div class="comments-container mt-10 w-full ">
    <div id="comment-anchor" class="w-full h-2.5"></div>
    <div class="comment-area-title w-full my-1.5 md:my-2.5 text-xl md:text-3xl font-bold">
        Comments
    </div>
    

        
            
    <div id="giscus-container"></div>
    <script data-swup-reload-script defer>
        async function loadGiscus() {
            const giscusConfig = {
                'src': 'https://giscus.app/client.js',
                'data-repo': 'jachinzhang1/jachinzhang1.github.io',
                'data-repo-id': 'R_kgDONy_q0w',
                'data-category': 'Announcements',
                'data-category-id': 'DIC_kwDONy_q084Cmo0P',
                'data-mapping': 'title',
                'data-strict': '0',
                'data-reactions-enabled': '1',
                'data-emit-metadata': '1',
                'data-theme': 'preferred_color_scheme',
                'data-lang': 'zh-CN',
                'data-input-position': 'top',
                'data-loading': 'lazy',
                'crossorigin': 'anonymous',
                'async': true
            }
            const giscusScript = document.createElement('script');
            for (const key in giscusConfig) {
                giscusScript.setAttribute(key, giscusConfig[key]);
            }
            document.getElementById('giscus-container').appendChild(giscusScript);
        }
        if ('true') {
            let loadGiscusTimeout = setTimeout(() => {
                loadGiscus();
                clearTimeout(loadGiscusTimeout);
            }, 1000);
        } else {
            document.addEventListener('DOMContentLoaded', loadGiscus);
        }
    </script>


        
        
    
</div>

		</div>
		
	</div>

	
	<div class="toc-content-container">
		<div class="post-toc-wrap">
	<div class="post-toc">
		<div class="toc-title">On this page</div>
		<div class="page-title">深度学习基础：基于DDPM模型的生成模型demo笔记（二）</div>
		<ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E6%83%85%E6%8F%90%E8%A6%81"><span class="nav-number">1.</span> <span class="nav-text">前情提要</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%95%E5%85%A5%E7%B1%BB%E5%88%AB%E6%A0%87%E7%AD%BE%E7%89%B9%E5%BE%81"><span class="nav-number">2.</span> <span class="nav-text">引入类别标签特征</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%BC%95%E5%85%A5"><span class="nav-number">3.</span> <span class="nav-text">自注意力机制引入</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4%E7%AD%96%E7%95%A5"><span class="nav-number">4.</span> <span class="nav-text">学习率调整策略</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%93%E6%9E%9C%E5%B1%95%E7%A4%BA%E4%B8%8E%E5%88%86%E6%9E%90"><span class="nav-number">5.</span> <span class="nav-text">结果展示与分析</span></a></li></ol>

	</div>
</div>
	</div>
	
</div>
			</div>

			
		</div>

		<div class="main-content-footer">
			<footer class="footer mt-5 py-5 h-auto text-base text-third-text-color relative border-t-2 border-t-border-color">
    <div class="info-container py-3 text-center">
        
        <div class="text-center">
            &copy;
            
              <span>2025</span>
              -
            
            2025&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 0.5s; color: #f54545"></i>&nbsp;&nbsp;<a href="/">Jachin Zhang</a>
            
        </div>
        
            <script data-swup-reload-script src="https://cn.vercount.one/js"></script>
            <div class="relative text-center lg:absolute lg:right-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-right">
                
                    <span id="busuanzi_container_site_uv" class="lg:!block">
                        <span class="text-sm">VISITOR COUNT</span>
                        <span id="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="lg:!block">
                        <span class="text-sm">TOTAL PAGE VIEWS</span>
                        <span id="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="relative text-center lg:absolute lg:left-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-left">
            <span class="lg:block text-sm">POWERED BY <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg class="relative top-[2px] inline-block align-baseline" version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" class="text-base" href="https://hexo.io">Hexo</a></span>
            <span class="text-sm lg:block">THEME&nbsp;<a class="text-base" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.8.2</a></span>
        </div>
        
        
            <div>
                Blog up for <span class="odometer" id="runtime_days" ></span> days <span class="odometer" id="runtime_hours"></span> hrs <span class="odometer" id="runtime_minutes"></span> Min <span class="odometer" id="runtime_seconds"></span> Sec
            </div>
        
        
            <script data-swup-reload-script>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
        
    </div>  
</footer>
		</div>
	</div>

	
	<div class="post-tools">
		<div class="post-tools-container">
	<ul class="article-tools-list">
		<!-- TOC aside toggle -->
		
		<li class="right-bottom-tools page-aside-toggle">
			<i class="fa-regular fa-outdent"></i>
		</li>
		

		<!-- go comment -->
		
		<li class="go-comment">
			<i class="fa-regular fa-comments"></i>
		</li>
		
	</ul>
</div>
	</div>
	

	<div class="right-side-tools-container">
		<div class="side-tools-container">
	<ul class="hidden-tools-list">
		<li class="right-bottom-tools tool-font-adjust-plus flex justify-center items-center">
			<i class="fa-regular fa-magnifying-glass-plus"></i>
		</li>

		<li class="right-bottom-tools tool-font-adjust-minus flex justify-center items-center">
			<i class="fa-regular fa-magnifying-glass-minus"></i>
		</li>

		<li class="right-bottom-tools tool-dark-light-toggle flex justify-center items-center">
			<i class="fa-regular fa-moon"></i>
		</li>

		<!-- rss -->
		

		

		<li class="right-bottom-tools tool-scroll-to-bottom flex justify-center items-center">
			<i class="fa-regular fa-arrow-down"></i>
		</li>
	</ul>

	<ul class="visible-tools-list">
		<li class="right-bottom-tools toggle-tools-list flex justify-center items-center">
			<i class="fa-regular fa-cog fa-spin"></i>
		</li>
		
		<li class="right-bottom-tools tool-scroll-to-top flex justify-center items-center">
			<i class="arrow-up fas fa-arrow-up"></i>
			<span class="percent"></span>
		</li>
		
		
	</ul>
</div>
	</div>

	<div class="image-viewer-container">
	<img src="">
</div>

	
	<div class="search-pop-overlay">
	<div class="popup search-popup">
		<div class="search-header">
			<span class="search-input-field-pre">
				<i class="fa-solid fa-keyboard"></i>
			</span>
			<div class="search-input-container">
				<input autocomplete="off" autocorrect="off" autocapitalize="off" placeholder="Search..." spellcheck="false" type="search" class="search-input">
			</div>
			<span class="popup-btn-close">
				<i class="fa-solid fa-times"></i>
			</span>
		</div>
		<div id="search-result">
			<div id="no-result">
				<i class="fa-solid fa-spinner fa-spin-pulse fa-5x fa-fw"></i>
			</div>
		</div>
	</div>
</div>
	

</main>



<script src="/js/build/libs/Swup.min.js"></script>

<script src="/js/build/libs/SwupSlideTheme.min.js"></script>

<script src="/js/build/libs/SwupScriptsPlugin.min.js"></script>

<script src="/js/build/libs/SwupProgressPlugin.min.js"></script>

<script src="/js/build/libs/SwupScrollPlugin.min.js"></script>

<script src="/js/build/libs/SwupPreloadPlugin.min.js"></script>

<script>
    const swup = new Swup({
        plugins: [
            new SwupScriptsPlugin({
                optin: true,
            }),
            new SwupProgressPlugin(),
            new SwupScrollPlugin({
                offset: 80,
            }),
            new SwupSlideTheme({
                mainElement: ".main-content-body",
            }),
            new SwupPreloadPlugin(),
        ],
        containers: ["#swup"],
    });
</script>





    

  
  

	
<script src="/js/build/tools/imageViewer.js" type="module"></script>

<script src="/js/build/utils.js" type="module"></script>

<script src="/js/build/main.js" type="module"></script>

<script src="/js/build/layouts/navbarShrink.js" type="module"></script>

<script src="/js/build/tools/scrollTopBottom.js" type="module"></script>

<script src="/js/build/tools/lightDarkSwitch.js" type="module"></script>

<script src="/js/build/layouts/categoryList.js" type="module"></script>



    
<script src="/js/build/tools/localSearch.js" type="module"></script>




    
<script src="/js/build/tools/codeBlock.js" type="module"></script>




    
<script src="/js/build/layouts/lazyload.js" type="module"></script>




    
<script src="/js/build/tools/runtime.js"></script>

    
<script src="/js/build/libs/odometer.min.js"></script>

    
<link rel="stylesheet" href="/assets/odometer-theme-minimal.css">




  
<script src="/js/build/libs/Typed.min.js"></script>

  
<script src="/js/build/plugins/typed.js" type="module"></script>




    
        
<script src="/js/build/libs/mermaid.min.js"></script>

    
    
<script src="/js/build/plugins/mermaid.js"></script>




    
<script src="/js/build/libs/minimasonry.min.js"></script>

    
<script src="/js/build/plugins/masonry.js" type="module"></script>







    
<script src="/js/build/tools/tocToggle.js" type="module" data-swup-reload-script=""></script>

<script src="/js/build/layouts/toc.js" type="module" data-swup-reload-script=""></script>

<script src="/js/build/plugins/tabs.js" type="module" data-swup-reload-script=""></script>




<script src="/js/build/libs/moment-with-locales.min.js" data-swup-reload-script=""></script>


<script src="/js/build/layouts/essays.js" type="module" data-swup-reload-script=""></script>





	
</body>

</html>